{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlzfQumgj7rh",
        "outputId": "f170a242-e5d1-4b5e-c7ff-00d8db385909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hcUbmcjj_iF",
        "outputId": "1eb85fc7-81a0-4e59-9a60-2c2f8bd2480b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opacus\n",
            "  Downloading opacus-1.2.0-py3-none-any.whl (203 kB)\n",
            "\u001b[K     |████████████████████████████████| 203 kB 30.2 MB/s \n",
            "\u001b[?25hCollecting functorch\n",
            "  Downloading functorch-1.13.0-py2.py3-none-any.whl (2.1 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8->opacus) (4.1.1)\n",
            "Collecting torch>=1.8\n",
            "  Downloading torch-1.13.0-cp37-cp37m-manylinux1_x86_64.whl (890.2 MB)\n",
            "\u001b[K     |██████████████████████████████  | 834.1 MB 1.2 MB/s eta 0:00:47tcmalloc: large alloc 1147494400 bytes == 0x3a6f4000 @  0x7f597acd1615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████████████████████| 890.2 MB 6.8 kB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 557.1 MB 12 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 62.4 MB/s \n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 317.1 MB 31 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8->opacus) (0.37.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8->opacus) (57.4.0)\n",
            "Installing collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, torch, functorch, opacus\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\n",
            "Successfully installed functorch-1.13.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 opacus-1.2.0 torch-1.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install opacus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-pp5OiNN3jk",
        "outputId": "4b7318b0-a2bf-4197-e8b3-7859dbe95d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-2.1.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from medmnist) (4.64.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.0.2)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 217 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.13.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from medmnist) (7.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from medmnist) (0.13.1+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.3.5)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from medmnist) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire->medmnist) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->medmnist) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->medmnist) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (2.9.0)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (1.7.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (2021.11.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->medmnist) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->medmnist) (1.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.7/dist-packages (from torch->medmnist) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.7/dist-packages (from torch->medmnist) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.7/dist-packages (from torch->medmnist) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.7/dist-packages (from torch->medmnist) (8.5.0.96)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->medmnist) (0.37.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->medmnist) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->medmnist) (2.23.0)\n",
            "Collecting torch\n",
            "  Downloading torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.3 MB 12 kB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (1.24.3)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=7e00b3c96c92ec942a500007b0821e94f117e71a43cbc9879ddf666d345a540b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: torch, fire, medmnist\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0\n",
            "    Uninstalling torch-1.13.0:\n",
            "      Successfully uninstalled torch-1.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "functorch 1.13.0 requires torch<1.13.1,>=1.13.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\n",
            "Successfully installed fire-0.4.0 medmnist-2.1.0 torch-1.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6fDI-ulbJmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ddee5c-1fc8-4efc-bc3d-8b7dc3f470eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import optim, analysis, sampling\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "from torchvision.models import ResNeXt101_64X4D_Weights\n",
        "import torch.nn.functional as F\n",
        "#import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.dataset import Dataset \n",
        "from torch.utils.data import DataLoader, random_split  \n",
        "torch.backends.cudnn.benchmark=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zTi6YtPN8Dk"
      },
      "outputs": [],
      "source": [
        "data_flag = 'bloodmnist'\n",
        "download = True\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxerzR86bN6n"
      },
      "outputs": [],
      "source": [
        "##### Hyperparameters for federated learning #########\n",
        "batch_size = 64\n",
        "num_channels = 3\n",
        "data_dir = 'data/'\n",
        "\n",
        "params = {'l2_norm_clip': 1,\n",
        "          'noise_multiplier' :1.1,\n",
        "          'minibatch_size': 256,\n",
        "         'microbatch_size': 1,\n",
        "         'lr':0.15,\n",
        "         'l2_penalty' : 0,\n",
        "         'delta' : 1e-5,\n",
        "         'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "         'local_epochs' : 1,\n",
        "          \"total_clients\": 3,\n",
        "          \"num_sel\":3,\n",
        "          \"num_rounds\": 100}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt-BctUeQis6",
        "outputId": "40841198-8521-4de8-d568-e7f0b609d112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "49998\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_datasets():\n",
        "    # Download and transform CIFAR-10 (train and test)\n",
        "    # transform = transforms.Compose([\n",
        "    # transforms.Resize((224,224)),\n",
        "    # transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean  = (0.49139968, 0.48215827, 0.44653124), std = (0.24703233,\n",
        "    #                                                                           0.24348505, 0.26158768))])\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "     transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
        "\n",
        "    #train_dataset = datasets.OxfordIIITPet(root='/data', split = 'trainval', download=True, transform=transform)\n",
        "    #test_dataset = datasets.OxfordIIITPet(root='/data', split = 'test', download=True, transform=transform)\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(data_dir, train=True, transform=transform, download=True)\n",
        "    test_dataset = datasets.CIFAR10(data_dir, train=False, transform=transform, download=True)\n",
        "\n",
        "\n",
        "    ######################### NonIID #######################\n",
        "    # import numpy as np\n",
        "    # from torch.utils.data import Dataset,Subset\n",
        "    # labels = np.array(train_dataset.targets)\n",
        "\n",
        "    # idx = np.where(labels < 3)[0]\n",
        "    # idx2 = np.where((labels >= 3) & (labels <= 6))[0]\n",
        "    # idx3 = np.where(labels > 6)[0]\n",
        "\n",
        "\n",
        "    # train_datasets= []\n",
        "    # #print(idx)\n",
        "    # train_datasets.append(Subset(train_dataset,idx))\n",
        "    # train_datasets.append(Subset(train_dataset,idx2))\n",
        "    # train_datasets.append(Subset(train_dataset,idx3))\n",
        "    ############################################################################################\n",
        "\n",
        "    # train_dataset = datasets.SVHN(data_dir, split = 'train', transform=transform, download=True)\n",
        "    # test_dataset = datasets.SVHN(data_dir, split = 'test', transform=transform, download=True)\n",
        "\n",
        "\n",
        "    # train_dataset = DataClass(split='train', transform=transform, download=download)\n",
        "    # test_dataset  = DataClass(split='test', transform=transform, download=download)\n",
        "\n",
        "    # Split training set into clients to simulate the individual dataset\n",
        "    #partition_size = len(train_dataset) // params['total_clients']\n",
        "    #lengths = [partition_size] * params['total_clients'] ## This line may need more explanation\n",
        "\n",
        "\n",
        "    train_datasets = random_split(train_dataset, [len(train_dataset)-2,2])\n",
        "    print(len(train_datasets[0]))\n",
        "    #print(int((len(train_datasets[0])))//3)\n",
        "    train_datasets = random_split(train_datasets[0], [int((len(train_dataset))/ params['total_clients']) for _ in range(params['total_clients'])])\n",
        "    \n",
        "    #train_datasets = random_split(train_dataset, [int(train_dataset.data.shape[0] / params['total_clients']) for _ in range(params['total_clients'])])\n",
        "\n",
        "    \n",
        "    return train_datasets, test_dataset\n",
        "\n",
        "train_datasets, test_dataset = load_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pit6D0VDRBVh"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxQKo__PijB_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8xmE__eQ2y0"
      },
      "outputs": [],
      "source": [
        "class MNISTResNet(nn.Module):\n",
        "        def __init__(self, in_channels=1):\n",
        "                super(MNISTResNet, self).__init__()\n",
        "                # loading a pretrained model\n",
        "                self.model = torchvision.models.resnext101_64x4d(ResNeXt101_64X4D_Weights)\n",
        "                # changing the input color channels to 1 since original resnet has 3 channels for RGB\n",
        "                #self.model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "                #change the output layer to 10 ckasses as the original resnet has 1000 classes\n",
        "                num_ftrs = self.model.fc.in_features\n",
        "                self.model.fc = nn.Linear(num_ftrs, 10)\n",
        "                for name,param in self.model.named_parameters():\n",
        "                    if(name == 'fc.weight'):\n",
        "                        param.requires_grad =True\n",
        "                    elif(name =='fc.bias'):\n",
        "                        param.requires_grad = True\n",
        "                    else:\n",
        "                        param.requires_grad = False\n",
        "        def forward(self, t):\n",
        "                return self.model(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SYJIahw5Tae"
      },
      "outputs": [],
      "source": [
        "def client_update(client_model, train_dataset):\n",
        "  \n",
        "  \n",
        "  classifier = client_model\n",
        "\n",
        "  optimizer = optim.DPSGD(\n",
        "        l2_norm_clip=params['l2_norm_clip'],\n",
        "        noise_multiplier=params['noise_multiplier'],\n",
        "        minibatch_size=params['minibatch_size'],\n",
        "        microbatch_size=params['microbatch_size'],\n",
        "        params = filter(lambda p: p.requires_grad, classifier.parameters()),\n",
        "        lr=params['lr'],\n",
        "        weight_decay=params['l2_penalty'],\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    #print('Achieves ({}, {})-DP'.format(analysis.epsilon(len(train_dataset),params['minibatch_size'],params['noise_multiplier'],params['iterations'],params['delta']),params['delta'],))\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  minibatch_loader, microbatch_loader = sampling.get_data_loaders(\n",
        "        params['minibatch_size'],\n",
        "        params['microbatch_size'],\n",
        "        params['local_epochs']\n",
        "    )\n",
        "\n",
        "    #iteration = 0\n",
        "    #acc = []\n",
        "  for X_minibatch, y_minibatch in minibatch_loader(train_dataset):\n",
        "        optimizer.zero_grad()\n",
        "        y_minibatch = torch.squeeze(y_minibatch)\n",
        "        for X_microbatch, y_microbatch in microbatch_loader(TensorDataset(X_minibatch, y_minibatch)):\n",
        "            \n",
        "            # weights = ResNet50_Weights.DEFAULT\n",
        "            # preprocess = weights.transforms()\n",
        "            # X_microbatch = preprocess(X_microbatch)\n",
        "            X_microbatch = X_microbatch.to(params['device'])\n",
        "            y_microbatch = y_microbatch.to(params['device'])\n",
        "\n",
        "            optimizer.zero_microbatch_grad()\n",
        "            loss = loss_function(classifier(X_microbatch), y_microbatch)\n",
        "            loss.backward()\n",
        "            optimizer.microbatch_step()\n",
        "        optimizer.step()\n",
        "\n",
        "        # if iteration % 10 == 0:\n",
        "        #     acc.append(test())\n",
        "        #     print('Achieves ({}, {})-DP'.format(analysis.epsilon(len(train_dataset),params['minibatch_size'],params['noise_multiplier'],iteration,params['delta']),params['delta'],))\n",
        "        #     print('[Iteration %d/%d] [Loss: %f]' % (iteration, params['iterations'], loss.item()))\n",
        "\n",
        "        #iteration += 1\n",
        "\n",
        "    #return classifier,acc\n",
        "  return classifier,loss.item() #,val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAtTgU6-4X9v"
      },
      "outputs": [],
      "source": [
        "\n",
        "def server_aggregate(global_model, client_models):\n",
        "    \"\"\"\n",
        "    This function has aggregation method 'mean'\n",
        "    \"\"\"\n",
        "\n",
        "    global_dict = global_model.state_dict()\n",
        "    for k in global_dict.keys(): \n",
        "      global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
        "      global_model.load_state_dict(global_dict)\n",
        "    \n",
        "    global_model.load_state_dict(global_dict)\n",
        "    for model in client_models:\n",
        "      model.load_state_dict(global_model.state_dict())\n",
        "    \n",
        "    return client_models\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7y3Bm65lIjM",
        "outputId": "3f13741a-9a42-4c89-b607-b5896379369a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNFYeW-aSRN0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test(model):\n",
        "\n",
        "  from torchmetrics import AUROC\n",
        "  from statistics import mean\n",
        "  import torch.nn.functional as F\n",
        "  classifier = model\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "  classifier.eval()\n",
        "  accuracy = 0.0\n",
        "  total = 0.0\n",
        "  a = []\n",
        "  with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images = images.to(params['device'])\n",
        "            labels = torch.squeeze(labels)\n",
        "            labels = labels.to(params['device'])\n",
        "            \n",
        "            # run the model on the test set to predict labels\n",
        "            outputs = classifier(images)\n",
        "            #print(outputs.shape[1])\n",
        "            probs = F.softmax(outputs)\n",
        "            # the label with the highest energy will be our prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            auroc = AUROC(pos_label=None,num_classes = outputs.shape[1])\n",
        "            a.append(float(auroc(probs,labels).detach().cpu().numpy()))\n",
        "            total += labels.size(0)\n",
        "            accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "  #print(mean(a))\n",
        "  accuracy = (100 * accuracy / total)\n",
        "  print('Test Accuracy: {}'.format(accuracy),'                                      AUROC: {}'.format(mean(a)))\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8V8qg7ZK4-p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "9106d9ac-43ab-4ccc-b55e-c7fc35607cb3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-745b6378310b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopacus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopacus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModuleValidator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mglobal_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mMNISTResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mglobal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModuleValidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_model\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mModuleValidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_model\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-154-b0e5fc8369b1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels)\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNISTResNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0;31m# loading a pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnext101_64x4d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResNeXt101_64X4D_Weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0;31m# changing the input color channels to 1 since original resnet has 3 channels for RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;31m#self.model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: resnext101_64x4d() takes 0 positional arguments but 1 was given"
          ]
        }
      ],
      "source": [
        "############################################\n",
        "#### Initializing models and optimizer  ####\n",
        "############################################\n",
        "\n",
        "#### global model ##########\n",
        "import opacus\n",
        "from opacus.validators import ModuleValidator\n",
        "global_model =  MNISTResNet()\n",
        "global_model = ModuleValidator.fix(global_model ).to(params['device'])\n",
        "ModuleValidator.validate(global_model , strict=False)\n",
        "\n",
        "############## client models ##############\n",
        "client_models = [ ModuleValidator.fix(MNISTResNet()).to(params['device']) for _ in range(params[\"num_sel\"])]\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global model \n",
        "\n",
        "############### optimizers ################\n",
        "#opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95cjuxXnFIN8"
      },
      "outputs": [],
      "source": [
        "#client_models = server_aggregate(global_model, client_models)\n",
        "parameter_count = 0\n",
        "for p in global_model.parameters():\n",
        "  if(p.requires_grad):\n",
        "    #print(torch.numel(p))\n",
        "    parameter_count+=torch.numel(p)\n",
        "    print(p.size())\n",
        "\n",
        "print(parameter_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZwXV6Zd6rO3"
      },
      "outputs": [],
      "source": [
        "\n",
        "#net = global_model.to(device)\n",
        "\n",
        "# for epoch in range(200):\n",
        "#     global_model,loss= client_update(global_model, train_datasets[0])\n",
        "#     if(epoch%10==0):\n",
        "#       acc = test(global_model)\n",
        "#       print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {acc}\")\n",
        "   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS5I6gvNSabe"
      },
      "outputs": [],
      "source": [
        "#  client_idx = np.random.permutation(params[\"total_clients\"])[:params[\"num_sel\"]]\n",
        "#  print(client_idx )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP7Gp94LJxTc"
      },
      "outputs": [],
      "source": [
        "losses_train = []\n",
        "losses_test = []\n",
        "acc_train = []\n",
        "acc_test = []\n",
        "\n",
        "for r in range(params[\"num_rounds\"]):\n",
        "    # select random clients\n",
        "    # \"total_clients\": 4,\n",
        "    #       \"num_sel\":4,\n",
        "    client_idx = np.random.permutation(params[\"total_clients\"])[:params[\"num_sel\"]]\n",
        "    #client_idx =[1,2,3]\n",
        "    # client update\n",
        "    loss = 0\n",
        "    temp = 0\n",
        "    temp1 = 0\n",
        "    val_acc = 0\n",
        "    for i in tqdm(range(params[\"num_sel\"])):\n",
        "      for j in tqdm(range(params['local_epochs'])):\n",
        "        client_models[i],loss_temp = client_update(client_models[i], train_datasets[client_idx[i]])\n",
        "      client_models[i].eval()\n",
        "      loss+=loss_temp \n",
        "    losses_train.append(loss)\n",
        "    losses_train.append(loss)\n",
        "    # server aggregate\n",
        "    client_models = server_aggregate(global_model, client_models)\n",
        "\n",
        "    # _,val_acc = test(global_model,valloaders[0])\n",
        "\n",
        "    if(r%10==0):\n",
        "      acc = test(global_model)\n",
        "\n",
        "      acc_test.append(acc)\n",
        "      print('%d-th round' % r)\n",
        "      print('average train loss %0.3g  | test acc: %0.3f' % (loss / params[\"num_sel\"], acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAYgso41PYGB"
      },
      "outputs": [],
      "source": [
        "a = test(global_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep3J8IVf3WCQ"
      },
      "outputs": [],
      "source": [
        "acc_test.append(test(global_model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSe93wQW62nU"
      },
      "outputs": [],
      "source": [
        "acc_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8uQ8vgc7RyF"
      },
      "outputs": [],
      "source": [
        "b=[]\n",
        "for i in range(len(a)):\n",
        "  #print(a[i].detach().cpu().numpy())\n",
        "  b.append(float(a[i].detach().cpu().numpy()))\n",
        "print(mean(b))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "id": "BnAtvee5sLWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3E3863ySsx2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}